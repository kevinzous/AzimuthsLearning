{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# ploting\n",
    "'''from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "# modeling\n",
    "import statsmodels.formula.api as smf\n",
    "'''import statsmodels.api as sm\n",
    "import statsmodels.tsa.arima_process as sta\n",
    "import statsmodels.graphics.tsaplots as sgt\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import statsmodels.tsa.statespace as sts\n",
    "'''\n",
    "from sklearn import preprocessing,metrics , model_selection\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline solution 1 : mean (score : 1.69158)\n",
    "'''\n",
    "sales_train_validation, calendar, sell_prices, sample_submission = read_data('data')\n",
    "sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "\n",
    "mean_sales=sales_train_validation.mean(axis=1,numeric_only=True)\n",
    "mean=mean_sales.append(mean_sales).reset_index(drop=True)\n",
    "for col in ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n",
    "          'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', \n",
    "          'F20', 'F21', 'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']:\n",
    "    sample_submission[col]=mean  \n",
    "    \n",
    "sample_submission.to_csv(\"submission_mean.csv\",index=False)\n",
    "'''\n",
    "# baseline solution 2 : zeros before trimmed + mean (score : 1.20619)\n",
    "'''\n",
    "sales_train_validation, calendar, sell_prices, sample_submission = read_data('data')\n",
    "sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "\n",
    "my_array=sales_train_validation.iloc[:,6:].values\n",
    "sub=[np.mean(np.trim_zeros(row)) for row in my_array]\n",
    "for col in ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n",
    "          'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', \n",
    "          'F20', 'F21', 'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']:\n",
    "    sample_submission[col]=sub+sub  \n",
    "    \n",
    "sample_submission.to_csv(\"submission_mean_wo_zero.csv\",index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Reading data and data manipulation (melt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**WE FIRST WILL ONLY TAKE FIRST 10000 TIME SERIES TO EASE THE LOAD ON CPU/RAM **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\" path : string \"\"\"\n",
    "    df1 = pd.read_csv(path+'/sales_train_validation_last_year.csv') # dropped to have only last y cols\n",
    "                                                                    # sales_train_validation_last_year = sales_train_validation.drop(columns=['d_'+str(i) for i in range(1,1547)])\n",
    "    df2 = pd.read_csv(path+'/calendar.csv',parse_dates=[0])\n",
    "    df3 = pd.read_csv(path+'/sell_prices.csv')\n",
    "    df4 = pd.read_csv('data/sample_submission.csv')\n",
    "    return df1,df2,df3,df4\n",
    "\n",
    "def reduce_memory(dfs,verbose=False):\n",
    "    return [reduce_memory_usage(df) for df in dfs]\n",
    "    \n",
    "def reduce_memory_usage(df, verbose=False):\n",
    "    \"\"\"reduce memory usage of integer & float columns based on their value range\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem)\n",
    "             )\n",
    "    return df\n",
    "\n",
    "def data_manipulation(sales_train_validation,n_samples=10000):\n",
    "    sales_train_validation = sales_train_validation[:n_samples]\n",
    "    # unpivote the columns \n",
    "    value_vars=sales_train_validation.columns.to_list()[6:]  # remove the first 6 variables ie 'id','item_id', 'dept_id', 'cat_id', 'store_id','state_id'\n",
    "    id_vars=['id','item_id', 'dept_id', 'cat_id', 'store_id','state_id']\n",
    "    sales = pd.melt(sales_train_validation,id_vars=id_vars, value_vars=value_vars,var_name='d', value_name='sales_count')\n",
    "    \n",
    "    # join with calendar\n",
    "    sales=pd.merge(sales,calendar,left_on='d',right_on='d',how=\"left\")\n",
    "    # join with selling price\n",
    "    # seperate test dataframes\n",
    "    '''sales_validation_id = [row for row in sample_submission['id'] if 'validation' in row]\n",
    "    sales_evaluation_id = [row for row in sample_submission['id'] if 'evaluation' in row]\n",
    "\n",
    "    sales_validation = sample_submission[sample_submission['id'].isin(sales_validation_id)]\n",
    "    sales_evaluation = sample_submission[sample_submission['id'].isin(sales_evaluation_id)]\n",
    "    '''\n",
    "    # free memory\n",
    "    del id_vars,value_vars\n",
    "    return sales #,sales_validation,sales_evaluation\n",
    "\n",
    "def feature_engineering(sales):\n",
    "    # drop useless columns\n",
    "    useless_cols=[\"d\",\"wm_yr_wk\",\"wday\",\"weekday\"]\n",
    "    sales.drop(columns=useless_cols,inplace=True)\n",
    "    \n",
    "    # fill nan\n",
    "    nan_cols=[\"event_name_1\",\"event_type_1\",\"event_name_2\",\"event_type_2\"]\n",
    "    for col in nan_cols:\n",
    "        sales[col].fillna('Nothing',inplace=True)\n",
    "    \n",
    "    # encode categorical\n",
    "    '''cat_cols=[\"item_id\",\"dept_id\",\"store_id\",\"state_id\",\"cat_id\",\"weekday\"]+nan_cols\n",
    "    encoder = preprocessing.OneHotEncoder()\n",
    "    for col in cat_cols:\n",
    "        sales[col]= encoder.fit_transform(sales[col])''' \n",
    "\n",
    "    # feature engineering\n",
    "    '''source : https://kanoki.org/2019/09/09/how-to-shift-a-column-in-pandas/ \n",
    "    => use groupby + transform to see the separate value for each group.'''\n",
    "    \n",
    "    # sales series features\n",
    "    '''1- groupby id (ie timeseries (10000 series)\n",
    "       2- take the sales_count of 28 days before of that same series\n",
    "       3- the sales found at column lag_28 in row i will be found in the column sales_count at 28*n_sample + i \n",
    "       4- temp.iloc[28*10000+i]==sales.iloc[i,6]'''\n",
    "    sales['lag_1d']=sales.groupby(['id'])['sales_count'].transform(lambda x: x.shift(1))\n",
    "    sales['lag_2d']=sales.groupby(['id'])['sales_count'].transform(lambda x: x.shift(2))\n",
    "    sales['lag_3d']=sales.groupby(['id'])['sales_count'].transform(lambda x: x.shift(3))\n",
    "    sales['lag_1w']=sales.groupby(['id'])['sales_count'].transform(lambda x: x.shift(7))\n",
    "    sales['lag_2w']=sales.groupby(['id'])['sales_count'].transform(lambda x: x.shift(14))\n",
    "    sales['lag_4w']=sales.groupby(['id'])['sales_count'].transform(lambda x: x.shift(28))\n",
    "    \n",
    "    sales['rolling_avg_1w']=sales.groupby(['id'])['sales_count'].transform(lambda x : x.rolling(7,min_periods=1).mean())\n",
    "    sales['rolling_avg_1m']=sales.groupby(['id'])['sales_count'].transform(lambda x : x.rolling(30,min_periods=1).mean())\n",
    "    sales['rolling_avg_6m']=sales.groupby(['id'])['sales_count'].transform(lambda x : x.rolling(180,min_periods=1).mean())\n",
    "    sales['rolling_avg_1y']=sales.groupby(['id'])['sales_count'].transform(lambda x : x.rolling(365,min_periods=1).mean())\n",
    "    \n",
    "    sales['rolling_std_1w']=sales.groupby(['id'])['sales_count'].transform(lambda x : x.rolling(7,min_periods=1).std())\n",
    "    sales['rolling_std_1m']=sales.groupby(['id'])['sales_count'].transform(lambda x : x.rolling(30,min_periods=1).std())\n",
    "    \n",
    "    # time features \n",
    "    date_cols=[\"date\",\"year\",\"month\"]\n",
    "    sales['dayofweek']=sales.date.dt.dayofweek\n",
    "    sales['dayofmonth']=sales.date.dt.day\n",
    "    \n",
    "    \n",
    "    # drop nan due to the rolling function\n",
    "    initial_len=len(sales)\n",
    "    sales.dropna(inplace=True)\n",
    "    print('Dropped',initial_len-len(sales), 'rows out of', initial_len, 'initially')\n",
    "    \n",
    "    sales = reduce_memory_usage(sales,verbose=True)\n",
    "    return sales\n",
    "\n",
    "def model_fitting(y, feature_set, sales):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    formula = y + '~' + '+'.join(feature_set)\n",
    "    # fit the regression model\n",
    "    model = smf.ols(formula=formula, data=sales).fit()\n",
    "    return model\n",
    "def model_eval(target, feature_set, sales):\n",
    "    y=sales[target]\n",
    "    X=sales.drop(columns=[target])\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    X_train[target]=y_train\n",
    "    model=model_fitting(target, feature_set, X_train)\n",
    "    train_score = np.sqrt(metrics.mean_squared_error(model.predict(X_train),y_train))\n",
    "    print(\"train :\",train_score)\n",
    "    test_score = np.sqrt(metrics.mean_squared_error(model.predict(X_test),y_test))    \n",
    "    print(\"test :\",test_score)\n",
    "    return model,train_score,test_score\n",
    "\n",
    "def predict(test):\n",
    "    predictions = test[['id', 'date', 'demand']]\n",
    "    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "    evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "    validation = submission[['id']].merge(predictions, on = 'id')\n",
    "    final = pd.concat([validation, evaluation])\n",
    "    final.to_csv('submission.csv', index = False)\n",
    "    \n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "start = time.time()\n",
    "sales_train_validation, calendar, sell_prices, sample_submission = read_data('data')\n",
    "sales_train_validation, calendar, sell_prices = reduce_memory([sales_train_validation, calendar, sell_prices])\n",
    "end = time.time()\n",
    "print(\"it has taken\",end-start,\"seconds for data loading \") # 210 s  - 9s\n",
    "###########################################################\n",
    "start = time.time()\n",
    "sales = data_manipulation(sales_train_validation,n_samples=len(sales_train_validation))    \n",
    "end = time.time()\n",
    "print(\"it has taken\",end-start,\"seconds for data manipulation\") # 15 s - 25 s\n",
    "###########################################################\n",
    "start = time.time()\n",
    "sales = feature_engineering(sales)\n",
    "end = time.time()\n",
    "print(\"it has taken\",end-start,\"seconds\") #  235 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'data'\n",
    "sales = pd.read_csv(path+'/features_sales_half_last_year.csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','month', 'event_name_1',\n",
    "                        'event_type_1','event_name_2', 'event_type_2','dayofweek']\n",
    "categorical_features = ['C('+feature+')' for feature in categorical_features]\n",
    "numerical_features = ['snap_CA', 'snap_TX', 'snap_WI','lag_1d', 'lag_2d', 'lag_3d', 'lag_1w', 'lag_2w',\n",
    "                      'lag_4w', 'rolling_avg_1w', 'rolling_avg_1m', 'rolling_avg_6m', 'rolling_avg_1y',\n",
    "                      'rolling_std_1w', 'rolling_std_1m','dayofmonth','year']\n",
    "                      # eventually move to categorical :  dayofmonth, year\n",
    "\n",
    "feature_set = categorical_features + numerical_features\n",
    "\n",
    "time_features = ['lag_1d', 'lag_2d', 'lag_3d', 'lag_1w', 'lag_2w','lag_4w', 'rolling_avg_1w','rolling_avg_1m',\n",
    "                 'rolling_avg_6m', 'rolling_std_1w', 'rolling_std_1m','rolling_avg_1y',#'C(store_id)',#'C(dept_id)', #'C(month)','year',\n",
    "                 'C(dayofweek)','dayofmonth','snap_CA', 'snap_TX', 'snap_WI']\n",
    "                # R2 = 0.575 for time_features and n_samples=1000\n",
    "                # R2 = 0.572 for numerical_features and n_samples=1000\n",
    "                # R2 = 0.731 for time_features and sales[:int(len(sales)/10)] wo month, year, avg_1y,\n",
    "                # R2 = 0.731 for time_features and sales[:int(len(sales)/10)] wo month, year, avg_1y, dept_id, store_id\n",
    "                # R2 = 0.759 for time_features and sales[:int(len(sales)/5)] wo month, year, avg_1y\n",
    "                # R2 = 0.759 for time_features and sales[:int(len(sales)/5)] wo month, year, avg_1y, dept_id, store_id\n",
    "                # R2 = 0.770 for time_features and sales[:int(len(sales)/3)] wo month, year, avg_1y, dept_id, store_id\n",
    "                # R2 = 0.771 for time_features and sales[:int(len(sales)/3)] wo month, year, avg_1y, dept_id, store_id\n",
    "                # cpu not enough for 1 y\n",
    "#model = model_fitting('sales_count', time_features, sales[:int(len(sales)/3)])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data=[],columns=['method',\"train_score\",\"test_score\",'feature_set','datasets','addi_details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=['Linear Regression',12,12,['fd','fd'],100/i,'']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FOODS        2270460\n",
       "HOUSEHOLD    1654260\n",
       "HOBBIES       892700\n",
       "Name: cat_id, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.cat_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % of the datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhou/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 2.044346243901079\n",
      "test : 1.9901372066602743\n",
      "20.0 % of the datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhou/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 2.1344193379556895\n",
      "test : 2.072108830536073\n",
      "33.333333333333336 % of the datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhou/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 2.157164052000654\n",
      "test : 2.145317692173323\n"
     ]
    }
   ],
   "source": [
    "time_features = [#lag_1d', 'lag_2d', 'lag_3d', 'lag_1w', 'lag_2w','lag_4w', 'rolling_avg_1w','rolling_avg_1m',\n",
    "                 'rolling_avg_6m', 'rolling_std_1w', 'rolling_std_1m',#'rolling_avg_1y',#'C(store_id)',#'C(dept_id)', #'C(month)','year',\n",
    "                 'C(dayofweek)','dayofmonth','snap_CA', 'snap_TX', 'snap_WI']\n",
    "                \n",
    "            ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','month', 'event_name_1',\n",
    "                 'event_type_1','event_name_2', 'event_type_2','dayofweek']\n",
    "\n",
    "categorical_features = ['item_id', 'dept_id', 'store_id', 'state_id','month', 'event_name_1',\n",
    "                        'event_type_1','event_name_2', 'event_type_2','dayofweek']\n",
    "categorical_features = ['C('+feature+')' for feature in categorical_features]\n",
    "\n",
    "for i in [10,5,3] :\n",
    "    for feature_set in [time_features]:\n",
    "        print(100/i,\"% of the datasets\")\n",
    "        model,train_score,test_score = model_eval('sales_count', feature_set, sales[:int(len(sales)/i)])\n",
    "        row=['Linear Regression',train_score,test_score,feature_set,100/i,'']\n",
    "        results.loc[loc]=row\n",
    "        loc+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>feature_set</th>\n",
       "      <th>datasets</th>\n",
       "      <th>addi_details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.80557</td>\n",
       "      <td>1.78015</td>\n",
       "      <td>[lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.80557</td>\n",
       "      <td>1.78015</td>\n",
       "      <td>[lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.83034</td>\n",
       "      <td>1.76586</td>\n",
       "      <td>[lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.83034</td>\n",
       "      <td>1.76586</td>\n",
       "      <td>[lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.86322</td>\n",
       "      <td>1.81045</td>\n",
       "      <td>[lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...</td>\n",
       "      <td>33.333333</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1.86322</td>\n",
       "      <td>1.81045</td>\n",
       "      <td>[lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...</td>\n",
       "      <td>33.333333</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              method train_score test_score  \\\n",
       "0  Linear Regression     1.80557    1.78015   \n",
       "1  Linear Regression     1.80557    1.78015   \n",
       "2  Linear Regression     1.83034    1.76586   \n",
       "3  Linear Regression     1.83034    1.76586   \n",
       "4  Linear Regression     1.86322    1.81045   \n",
       "5  Linear Regression     1.86322    1.81045   \n",
       "\n",
       "                                         feature_set   datasets addi_details  \n",
       "0  [lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...  10.000000               \n",
       "1  [lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...  10.000000               \n",
       "2  [lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...  20.000000               \n",
       "3  [lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...  20.000000               \n",
       "4  [lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...  33.333333               \n",
       "5  [lag_1d, lag_2d, lag_3d, lag_1w, lag_2w, lag_4...  33.333333               "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "''' \n",
    "test=pd.DataFrame([2,3,4,5,7])\n",
    "test['id']=['a','b','b','b','a']\n",
    "\n",
    "print(test)\n",
    "print(\"--------\")\n",
    "print(test.groupby(['id']).rolling(2,min_periods=1).mean())\n",
    "print(\"--------\")\n",
    "print(\"with transform: \")\n",
    "print(test.groupby(['id'])[0].transform(lambda x: x.rolling(2,min_periods=1).mean()))\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function needed for calculating interval of prediction\n",
    "    fit = modal \n",
    "    exog = new dataframe'''\n",
    "    \n",
    "def transform_exog_to_model(fit, exog):\n",
    "    transform=True\n",
    "    self=fit\n",
    "\n",
    "    # The following is lifted straight from statsmodels.base.model.Results.predict()\n",
    "    if transform and hasattr(self.model, 'formula') and exog is not None:\n",
    "        from patsy import dmatrix\n",
    "        exog = dmatrix(self.model.data.orig_exog.design_info.builder, exog)\n",
    "\n",
    "    if exog is not None:\n",
    "        exog = np.asarray(exog)\n",
    "        if exog.ndim == 1 and (self.model.exog.ndim == 1 or self.model.exog.shape[1] == 1):\n",
    "            exog = exog[:, None]\n",
    "        exog = np.atleast_2d(exog)  # needed in count model shape[1]\n",
    "\n",
    "    # end lifted code\n",
    "    return exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasttime=pd.Timestamp('2017-12-22 16:00:00')\n",
    "\n",
    "x_pred_index_no = range(40000,40500)\n",
    "x_pred_time = [lasttime+i*pd.Timedelta('1:00:00') for i in range (1, len(x_pred_index_no)+1)]\n",
    "\n",
    "newdf = pd.DataFrame(index=x_pred_time,columns=['index_no'], data= x_pred_index_no)\n",
    "\n",
    "newdf['year']=newdf.index.year-2012\n",
    "newdf['monthofyear']=newdf.index.month\n",
    "newdf['dayofmonth']=newdf.index.day\n",
    "newdf['dayofweek']=newdf.index.dayofweek\n",
    "newdf['hour']=newdf.index.hour\n",
    "\n",
    "y_pred = model.predict(newdf)\n",
    "transformed_exog = transform_exog_to_model(model, newdf)\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "prstd, iv_l, iv_u = wls_prediction_std(model, transformed_exog, weights=[1])\n",
    "\n",
    "train1_partial=train1['2017-12':]\n",
    "fig, ax = plt.subplots(figsize=(24, 6))\n",
    "ax.plot(train1_partial['index_no'], train1_partial['TrafficVolume'])\n",
    "ax.scatter(train1_partial['index_no'], train1_partial['TrafficVolume'])\n",
    "fig.suptitle('Prediction Intervals')\n",
    "ax.grid(True)\n",
    "ax.plot(list(x_pred_index_no), y_pred, '-', color='red', linewidth=2)\n",
    "# interval for observations\n",
    "ax.fill_between(x_pred_index_no, iv_l, iv_u, color='#888888', alpha=0.3)\n",
    "ax.axis('tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.85,
   "position": {
    "height": "143.95px",
    "left": "984px",
    "right": "20px",
    "top": "105px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
