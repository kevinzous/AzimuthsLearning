{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- Import the packages and reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "import datetime\n",
    "# ploting\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling\n",
    "'''\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.arima_process as sta\n",
    "import statsmodels.graphics.tsaplots as sgt\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import statsmodels.tsa.statespace as sts\n",
    "'''\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import sys\n",
    "\n",
    "# data reading\n",
    "sales_train_validation = pd.read_csv('data/sales_train_validation.csv')\n",
    "calendar = pd.read_csv('data/calendar.csv',parse_dates=[0])\n",
    "sell_prices = pd.read_csv('data/sell_prices.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**WE FIRST WILL ONLY TAKE 20 % OF THE DATA TO EASE THE LOAD ON CPU/RAM **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_validation = sales_train_validation.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 12.62 MB\n",
      "Decreased by 85.9%\n",
      "Memory usage after optimization is: 0.12 MB\n",
      "Decreased by 41.9%\n",
      "Memory usage after optimization is: 130.48 MB\n",
      "Decreased by 37.5%\n"
     ]
    }
   ],
   "source": [
    "#reduce memory usage\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "calendar = reduce_mem_usage(calendar)\n",
    "sell_prices = reduce_mem_usage(sell_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Data Exploration\n",
    "## 1-a) Nan Values\n",
    "* Nan values are essentially in the calendar datasets in the 'event_name_1', 'event_type_1', 'event_name_2','event_type_2' columns.\n",
    "  We suppose that nan here corresponds to day without any special event \n",
    "* The dataframe is however **very sparse** (lots of zeros as stated in the guidelines = > intermittency and sporadic demand )\n",
    "  On average, **68 %are zeros** and 50 % have more than 73% of zeros in their series ( median )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_train_validation nan values:  0\n",
      "sell_prices nan values:  0\n",
      "calendar nan values:  7542\n",
      "\n",
      "date               0\n",
      "wm_yr_wk           0\n",
      "weekday            0\n",
      "wday               0\n",
      "month              0\n",
      "year               0\n",
      "d                  0\n",
      "event_name_1    1807\n",
      "event_type_1    1807\n",
      "event_name_2    1964\n",
      "event_type_2    1964\n",
      "snap_CA            0\n",
      "snap_TX            0\n",
      "snap_WI            0\n",
      "dtype: int64\n",
      "\n",
      "count    6098.000000\n",
      "mean        0.678290\n",
      "std         0.226682\n",
      "min         0.002614\n",
      "25%         0.531626\n",
      "50%         0.733926\n",
      "75%         0.864088\n",
      "max         0.992682\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# missing values examination\n",
    "print(\"sales_train_validation nan values: \",sales_train_validation.isna().sum().sum())\n",
    "print(\"sell_prices nan values: \", sell_prices.isna().sum().sum())\n",
    "print(\"calendar nan values: \", calendar.isna().sum().sum())\n",
    "print() #line break \n",
    "\n",
    "print(calendar.isna().sum())\n",
    "print() #line break \n",
    "\n",
    "# data sparcity check\n",
    "percentage_zero=(sales_train_validation==0).sum(axis=1)/1913 ## 1913 is the number of days \n",
    "print(percentage_zero.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-b) Time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_vars=sales_train_validation.columns.to_list()[6:]  # remove the first 6 variables ie 'id','item_id', 'dept_id', 'cat_id', 'store_id','state_id'\n",
    "id_vars=['id','item_id', 'dept_id', 'cat_id', 'store_id','state_id']\n",
    "\n",
    "# unpivoting the columns \n",
    "sales = pd.melt(sales_train_validation,id_vars=id_vars, value_vars=value_vars,var_name='d', value_name='sales_count')\n",
    "# joining with calendar but memomry run out with this method \n",
    "sales=pd.merge(sales,calendar,left_on='d',right_on='d',how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot by month and year\n",
    "sales['month_year'] = sales['date'].apply(lambda x: x.replace(day=1)) \n",
    "# source : https://stackoverflow.com/questions/25146121/extracting-just-month-and-year-separately-from-pandas-datetime-column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "del id_vars,value_vars,sales_train_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. random plot\n",
    "* 2 same item_id can have different starting date and different bahaviours across different state\n",
    "* data before starting date NOT to take into account => ROLLING FEATURES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 # number of item_id to be plotted  \n",
    "plotted_sales = sales[sales['item_id'].isin(sales.item_id.tolist()[:n])]\n",
    "\n",
    "fig = px.line(plotted_sales, x=\"date\", y=\"sales_count\",\n",
    "        line_shape=\"spline\", render_mode=\"svg\",color=\"id\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii.aggregating sales by \n",
    "* date\n",
    "  * upward seasonal trend \n",
    "  * 0 sales for christmas with some exceptions most likely data input errors => to clean ? \n",
    "* state/store/product\n",
    "  * CA sales are higher (4 stores vs 3 in other states) \n",
    "  * across stores seasonal effects are different : ex in WI2 only, we see a significant        drop in Food sales at the end of each month (10 days) => feature  ? \n",
    "  * lowest on monday higest on thursday/friday\n",
    "  *   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total sales\n",
    "sales.groupby(by='date').sum()[['sales_count']].plot()\n",
    "\n",
    "# christmas sales \n",
    "sales.groupby(by='date').sum()['sales_count'].sort_values()\n",
    "# sales[(sales.event_name_1=='Christmas') & (sales.sales_count!=0)] # list is here\n",
    "\n",
    "# store sales\n",
    "fig = px.line(sales.groupby(by=['date','store_id'], as_index=False).sum(), x=\"date\", y=\"sales_count\",# color=\"continent\", line_group=\"country\", hover_name=\"country\",\n",
    "        line_shape=\"spline\", render_mode=\"svg\",color=\"store_id\")\n",
    "fig.show()\n",
    "#stores and categories \n",
    "fig = px.line(sales.groupby(by=['date','cat_id','store_id'], as_index=False).sum(), x=\"date\", y=\"sales_count\", facet_row=\"store_id\", color=\"cat_id\"\n",
    "                 #,category_orders={\"cat_id\": [\"Thur\", \"Fri\", \"Sat\", \"Sun\"], \"store_id\": [\"TX_2\",\"TX_1\",\"CA_3\",\"WI_2\",\"TX_3\",\"CA_4\",\"WI_1\",\"WI_3\",\"CA_2\",\"CA_1\"]}\n",
    "                ,width=800, height=1400)\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(sales.groupby(by=['month_year','dept_id','state_id'], as_index=False).sum(), x=\"month_year\", y=\"sales_count\", facet_row=\"state_id\",facet_col=\"dept_id\"\n",
    "                 ,width=1200, height=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate test dataframes\n",
    "sales_validation_id = [row for row in submission['id'] if 'validation' in row]\n",
    "sales_evaluation_id = [row for row in submission['id'] if 'evaluation' in row]\n",
    "\n",
    "sales_validation = submission[submission['id'].isin(sales_validation_id)]\n",
    "sales_evaluation = submission[submission['id'].isin(sales_evaluation_id)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
