
`Course : 1-Neural networks and deep learning` 

`Objectives :`
- Understand the major technology trends driving Deep Learning
- Be able to build, train and apply fully connected deep neural networks 
- Know how to implement efficient (vectorized) neural networks
- Understand the key parameters in a neural network's architecture

`Projects implemented:`
- Implementing Feed Forward algorithm and back-propagation algorithm from scratch
- Vectorization, Broadcasting and activation function
***

`Course : 2-Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization`	

`Objectives :`
- Understand industry best-practices for building deep learning applications. 
- Be able to effectively use the common neural network "tricks", including initialization (including Glorot's and He's initialization), L2 and dropout regularization, Batch normalization, gradient checking 
- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. 
- Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance
- Be able to implement a neural network in TensorFlow.	

`Projects implemented:`
- 2D-classifier to separate two non-linear geometrical forms 
(initialization)
- Find the positions on the field where the goalkeeper should kick the ball.
(regularÂ²ization)
- Implementation of optimization algorithm (mini-batch gradient descent, Momentum and Adam)
- Sign language deciphering (subset of number signs) with a neural network using Tensorflow
***


`Course : 3-Structure machine learning projects`	

`Objectives :`
- Set metrics and evaluate (data mismatch, reducible bias, variance)
- Understand how to diagnose errors in a machine learning system 
- Be able to prioritize the most promising directions for reducing error
- Understand complex ML settings, such as mismatched training/test sets, and comparing to and/or surpassing human-level performance
- Know how to apply end-to-end learning, transfer learning, and multi-task learning	
***

`Course : 4-Convolutional neural networks`

`Objectives :`
- Understand how to build a convolutional neural network (Convolutions, Padding, Max Pooling, Classical architecture like LeNet-5, AlexNet and VGG-16), including recent variations such as residual networks (they have feed forward skipped connections which allow you train extremely large networks without a drop in performance)
- Know how to apply convolutional networks to visual detection and recognition tasks.
- Know to use neural style transfer to generate art.	

`Projects implemented:`
***

`Course : 5-Sequence models`

`Objectives :`
- Understand how to build and train Recurrent Neural Networks and commonly-used variants such as GRUs and LSTMs 
- Be able to apply sequence models to natural language problems, including text synthesis. 
- Be able to apply sequence models to audio applications, including speech recognition and music synthesis.
- Use Keras	

`Projects implemented:`
- Building a Recurrent Neural Network - Step by Step (RNN cell, LSTM cell) 
- Character level language model: generate new dinosaur names using RNN cells
- Jazz music generation using LSTMs and keras
- Word embeddings operations using pretrained modals (Glove) : measurement of cosine similarity between 2 words,  Debiasing word vectors
- Build an Emojifier to have more expressive text messages using pretrained 50-dimensional GloVe embeddings + Avg + Softmax (v1) and LSTMs in Keras (v2)
- Neural Machine Translation (NTM) to translate human readable dates ("25th of June, 2009") into machine readable dates ("2009-06-25") with attention mechanism

- Trigger Word Detection: Creating a labeled speech dataset (audio clips of activates, negatives, and backgrounds)


***
